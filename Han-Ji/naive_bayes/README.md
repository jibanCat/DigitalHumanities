# Navie Bayes for Tagging

Having uncertainty is better than a point estimate. Assuming informative prior is better than giving a flat prior. As a (potential?) Bayesian, I believe "tagging," this standard pre-processing technique used in historian domain, could be expressed in a Bayesian way. 

Chinese historian tagging platform, like MARKUS, could not return a probability. Tagging platform expected users (historian, or humanities researcher) to use their knowledge to correct the tags manually. However, these platforms do encode some domain knowledge to classify tags (å®˜åã€åœ°åã€æ™‚é–“åã€äººå) in a heuristics way. So, why not encode this classification knowledge in a Bayesian way?

Without putting too much ML techniques on it, we can use Bayes rule to encode our domain knowledge and also return a probabilistic result. 

## Tag a time phrase
For example, consider the case to tag a time phrase:

```python
"éš†å®‰ä¸‰å¹´åä¸€æœˆ"
```

We can see there are some properties to identify in this phrase: å¹´è™Ÿ (éš†å®‰), æ•¸å­— (ä¸‰ã€åä¸€), å–®ä½ (å¹´ã€æœˆ). For this phrase, we can calculate the posterior probability of the phase being a time phrase given this phrase

$$
P(time \mid phrase) = \frac{ P(phrase \mid time) \times P(time) }{ P(phrase) }
$$

### Naive Bayes assumption

By Naive Bayes assumption, the likelehood $P(phrase \mid time)$ could be expressed a product of individual probability of each feature:

$$
P(phrase \mid time) = P(features \mid time) = P(feature_1 \mid time) \times ... \times P(feature_n \mid time) 
$$

For a time phrase in a Chinese historian text, we could build a feature vector based on our experience:


| is time | å¹´è™Ÿ | æ•¸å­— | å–®ä½ | å­£ç¯€ | å¹²æ”¯ | å§“æ° | ä¸ç›¸å¹²çš„å­— | 
| ----    | ---  | --- | --- | --- | --- | --- |  ----     |
| +       | 0.8  | 0.6 | 0.7 | 0.6 | 0.8 | 0.2 | 0.45      |
| -       | 0.2  | 0.4 | 0.3 | 0.4 | 0.2 | 0.8 | 0.55      |

the probs are given by the model in my brain ğŸ˜†. We could justify this model by collection large enough of time phrases.

The normalization factor is following

$$
P(phrase) = P(time) \prod{ P(feature_n \mid time)} + P(\sim time) \prod{ P(feature_n \mid \sim time)}
$$

Therefore, Bayes rule give you the posterior:
$$
P(time \mid phrase) = \frac{ P(time) \prod{ P(feature_n \mid time)} }{ P(time) \prod{ P(feature_n \mid time)} + P(\sim time) \prod{ P(feature_n \mid \sim time)} }
$$

> NOTE: å¹´è™Ÿã€æ•¸å­—ã€å–®ä½ã€å­£ç¯€ bags are copied from MARKUS, thanks MARKUS! 

## Examples:
```python
from naive_bayes_time import calc_time_posterior

calc_time_posterior("éš†å®‰ä¸‰å¹´åä¸€æœˆ") 
# 0.8873239436619719

calc_time_posterior("å¼µç„¡å¿Œå¸ä¸€å„„å¹´äºŒåä¸‰æœˆå­ä¸‘") 
# 0.4697469397630935

calc_time_posterior("å¸é¦¬å¤§äººæ˜¯å€‹äººç‰©ï¼Œä¸¦ä¸åªæ˜¯å¸é¦¬å¤§äººæ˜¯å€‹äººç‰©") 
# 0.002414582912800456
```

So, we are 89% certain that `"éš†å®‰ä¸‰å¹´åä¸€æœˆ"` is a time phrase, 0.2% certain `"å¸é¦¬å¤§äººæ˜¯å€‹äººç‰©ï¼Œä¸¦ä¸åªæ˜¯å¸é¦¬å¤§äººæ˜¯å€‹äººç‰©"` is a time phrase, and 47% certain `"å¼µç„¡å¿Œå¸ä¸€å„„å¹´äºŒåä¸‰æœˆå­ä¸‘"` is a time phrase. In practice, Navie Bayes allow us to pay more attention on `"å¼µç„¡å¿Œå¸ä¸€å„„å¹´äºŒåä¸‰æœˆå­ä¸‘"` pharse to further justify whether it is a time phrase or not. 